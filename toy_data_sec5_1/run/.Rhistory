#plot(wn)
#etav
#etavKL
#etao
#plot(xnew[1:1000,c(1,2)] %*% rep(1,2))
#hist( xnew[1:1000,c(1,2)] %*% rep(1,2) )
#plot(xnew[wn<1,c(1,2)],col = "red")
#points(xnew[wn>1,c(1,2)])
mu_true
beta
betastar
betastar = c(rep(1/d , (d-1)) , 1)
# Banana function
# https://arxiv.org/abs/0907.1254 Section 6
# Cornuet, Marin, Mira, Robert
# Adaptive Multiple Importance Sampling
# Scandinavian Journal of Statistics, 2012
f1=function(z,parameters){
xx=z
xx[,1] = z[,1] / sigphi
xx[,2] = z[,2]-bphi*(z[,1]**2-sigphi**2)
return(CompK(xx)/sigphi)
}
##f2 is the SMILE
f2=function(z){
#bphi=0
xx=z
xx[,1]=z[,1]/sigphi
xx[,2]=z[,2]-bphi*(z[,1]**2-sigphi**2)
return(.5*CompK(xx)/sigphi + .25*CompK(sweep(z,2,c1,FUN="-")/2)/2^d + .25*CompK(sweep(z,2,c2,FUN="-")/2)/2^d
)
}
#f3 is the Gaussian of the NIPS paper
f3 = function(x) {
yy = sweep(x,2,mu_true,FUN="-")
yy = sweep(yy,2,diag(sigma_true),'/')
return(CompK(yy) / prod(diag(sigma_true)))
}
#evaluation of normal density
CompK=function(z){
y = rowSums(z**2)/2
y = exp(-y)/(2*pi)**(dim(z)[2]/2)
return(y)
}
#eval of uniform density
CompK2=function(z){
return(ifelse(rowSums(-.5<=z&z<=.5)==d,1,0))
}
##for PAWL
logdensity_mixture <- function(x, parameters) log(  .5*dmvnorm(x, -parameters$mean, parameters$sd, log = FALSE) +
.5*dmvnorm(x, parameters$mean, parameters$sd, log = FALSE) )
mixture <- function(x, parameters)   p * dmvnorm(x, -parameters$mean, parameters$sd, log = FALSE) +
(1-p) * dmvnorm(x, parameters$mean, parameters$sd, log = FALSE)
logdensity_nips = function(x, parameters) dmvnorm(x, parameters$mean, parameters$sd, log = TRUE)
nips = function( x, parameters)  dmvnorm(x, parameters$mean, parameters$sd, log = FALSE)
rugby = function( x, parameters)  dmvnorm(x, parameters$mean, parameters$sd, log = FALSE)
####################################################################
################specify which f we take
if(func == "NIPS"){f = nips; logf = logdensity_nips
sig_mix = (.4 / sqrt(d))^2
mu_true = rep(5,d) / sqrt(d)
parameters = list(mean = mu_true, sd = diag(sig_mix,d))
mu_start = rep(0,d)
}
if(func == "banana"){f = f1
bphi = 0.03
sigphi = 10
mu_true = rep(0,d)
parameters = list(mean = mu_true)
mu_start = rep(0,d)
}
if(func == "rugby"){f = mixture;
p = 1/4
sig_mix = (.4 / sqrt(d))^2                                   #careful this is a variance
s = 1 / (2 * sqrt(d))
SIG =  diag(sig_mix,d)
SIG[1,1] = SIG[1,1] * 10
parameters <- list(mean = s * rep(1,d), sd = SIG)
mu_true = - parameters$mean * p + parameters$mean * (1-p)
if (d == 2) {a = NULL} else {a = rep(0,d-2)}
mu_start = c(1,-1, a) / sqrt(d)
} #vg2 = vg;sig2 = rep(sqrt(vg2),d);}
if(func == "mixture"){f = mixture;logf = logdensity_mixture
p=.5
sig_mix = (.4 / sqrt(d))^2                                   #careful this is a variance
s = 1 / (2 * sqrt(d))
mu_true = rep(0,d);
parameters <- list(mean = s * rep(1,d), sd = diag(sig_mix,d))
if (d == 2) {a = NULL} else {a = rep(0,d-2)}
mu_start = c(1,-1, a) / sqrt(d)
} #vg2 = vg;sig2 = rep(sqrt(vg2),d);}
if(func == "mixture2"){f = mixture;logf = logdensity_mixture
p = .5
sig_mix = (.4 / sqrt(d))^2                                   #careful this is a variance
s = 1 / (sqrt(d))
mu_true = rep(0,d);
parameters <- list(mean = s * rep(1,d), sd = diag(sig_mix,d))
if (d == 2) {a = NULL} else {a = rep(0,d-2)}
mu_start = c(1,-1, a) / sqrt(d)
} #vg2 = vg;sig2 = rep(sqrt(vg2),d);}
if(func == "smile"){f = f2; mu_true = rep(0,d)}
if(func == "NIPS2"){f = f3;sigf = 10; mu_true = rep(5,d); sigma_true = diag( c(sigf, rep(1,d-1) ));
vg = (5)^2 * (df-2) / df}
if(func == "logreg"){
mu_true = rep(0,d);
library(RCurl)
UCI_data_URL <- getURL('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data')
names <- c('id_number', 'diagnosis', 'radius_mean',
'texture_mean', 'perimeter_mean', 'area_mean',
'smoothness_mean', 'compactness_mean',
'concavity_mean','concave_points_mean',
'symmetry_mean', 'fractal_dimension_mean',
'radius_se', 'texture_se', 'perimeter_se',
'area_se', 'smoothness_se', 'compactness_se',
'concavity_se', 'concave_points_se',
'symmetry_se', 'fractal_dimension_se',
'radius_worst', 'texture_worst',
'perimeter_worst', 'area_worst',
'smoothness_worst', 'compactness_worst',
'concavity_worst', 'concave_points_worst',
'symmetry_worst', 'fractal_dimension_worst')
breast_cancer <- read.table(textConnection(UCI_data_URL), sep = ',', col.names = names)
breast_cancer$id_number <- NULL
n1 = sum(breast_cancer$diagnosis == "M")
n0 = sum(breast_cancer$diagnosis == "B")
breast_cancer$dianew = rep(0 ,n0+n1)
d = 31
ntot = dim(breast_cancer)[1]
breast_cancer$intercept = rep(1 ,ntot)
#breast_cancer [ breast_cancer$diagnosis == "B",] = rep(0 ,n0)
breast_cancer [ breast_cancer$diagnosis == "M",]$dianew = rep(1 ,n1)
breast_cancer_train = breast_cancer[(1:(ntot - 100)),]
breast_cancer_test = breast_cancer[((ntot-100 + 1 ) :ntot),]
ntrain = dim(breast_cancer_train)[1]
X_train =  breast_cancer_train [,-c(1,32)]
y_train = breast_cancer_train[,32]
X_test =  breast_cancer_test [,-c(1,32)]
y_test = breast_cancer_test[,32]
#dim(breast_cancer)
ntrain = 10000
ntest = 1000
d = 10
betastar = c(rep(1/d , (d-1)) , 1)
X_train = matrix(rnorm(ntrain*(d-1),0,1/sqrt (d) ),ntrain,(d-1))
X_train = cbind(X_train,rep(1,ntrain))
temp = colSums(betastar * t(X_train) )
p =  ( 1 / (1 + exp(-temp )))
y_train = rbinom(ntrain, 1, p)
X_test = matrix(rnorm(ntest*(d-1),0,1/sqrt (d) ), ntest, (d-1))
X_test = cbind(X_test,rep(1,ntest))
temp = colSums(betastar * t(X_test))
p =  ( 1 / (1 + exp(-temp  )))
y_test = rbinom(ntest, 1, p)
mu_true = betastar
#y_test = breast_cancer_test[,32]
#p = 1 / (1 + exp ( - rowSums(beta * X)  ) )
}
####################################################################
###########to use parallel package, the call is made using a function
IS_paral = function(i){
source("naive_is.R")
return(res)
}
AIS_paral = function(i){
source("AIS.R")
return(mem_error)
}
NPAIS_paral = function(i){
source("NPAIS.R")
return(res)
}
wl_paral = function(i){
source("wl.R")
return(chains)
}
AMH_paral = function(i){
source("AMH.R")
return(chains)
}
MH_paral = function(i){
source("MH.R")
return(chains)
}
mu_true
# ll = function(w,parameters){
#   temp = colSums(w[-1] * X)
#   temp = w[1] + temp
#
# return (sum(   c( -  temp[temp > 10]    + y[temp > 10] * temp[temp > 10],
#      - log (1 + exp ( temp[temp <= 10]) )   + y[temp <= 10] * temp[temp <= 10]) ))
# #return(  sum ( - log (1 + exp ( temp) )   + y * temp)  ) #dnorm ( w , rep(0,31), diag(1,31)) )
#   }
#
# beta = rep(1/32,31)
# ll(c(1,1))
# apply(rbind(beta,beta), 1, ll)
#f(xnew[1,])
#exp( f(w,parameters) + 200 )
S = sample(1:ntrain,100 , replace = TRUE)
dim(X_train)
Xsam = X_train [S,]
ysam = y_train[S]
dim(Xsam)
head(Xsam)
ll = function(w,parameters){
temp = colSums( w * t(Xsam))
#return (sum(   c( -  temp[temp > 10]    + y[temp > 10] * temp[temp > 10],
#                 - log (1 + exp ( temp[temp <= 10]) )   + y[temp <= 10] * temp[temp <= 10]) ))
return(  sum ( - log (1 + exp ( temp) )   + ysam * temp)  ) #dnorm ( w , rep(0,31), diag(1,31)) )
}
f = ll
source("init_NPAIS.R")
i = 1
iter = 0
source("log_reg.R")
#for (i in (1:Ttot)){
lambdat = lambdamem[i]
ht = rep(hmem[i],d)
#b = 1 / burnin
if (option == "new") {source("new_sampling.R")} else {source("expl_update_NPAIS_fast.R")}
#plot(wnorm)
#points(wnorm1)
######################save in a vector
rest=c(NPAIS_norm = err_norm , n_eff = n_eff, bdw = hmem[i], lbd = lambdat,etav)
res=rbind(res,rest)
######################update h and lambda
#sigma2 = (JUsquare/JUf - (Jmean_norm)^2) + rep(1,d) / sqrt(Nt[i])
#hthumb = (4/(d+2)) ** (1/(d+4)) * (Nt[i])^(-1/(d+4)) * sqrt(sigma2)
#}
plot(wn)
sum(wn>0)
res
i = i + 1
Jmean_norm
Jmean_current
beta = Jmean_norm
#  Jmean_current
#rbind(rep(1,31) , rep(-1,31)) * X_test[(1:2), ]
#beta = rep(1/d,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
#plot(xnew[1:1000,c(1)]+xnew[1:1000,c(2)])
#plot(xnew[1:1000,])
#plot(wn)
#etav
#etavKL
#etao
#plot(xnew[1:1000,c(1,2)] %*% rep(1,2))
#hist( xnew[1:1000,c(1,2)] %*% rep(1,2) )
#plot(xnew[wn<1,c(1,2)],col = "red")
#points(xnew[wn>1,c(1,2)])
source("log_reg.R")
#for (i in (1:Ttot)){
lambdat = lambdamem[i]
ht = rep(hmem[i],d)
#b = 1 / burnin
if (option == "new") {source("new_sampling.R")} else {source("expl_update_NPAIS_fast.R")}
#plot(wnorm)
#points(wnorm1)
######################save in a vector
rest=c(NPAIS_norm = err_norm , n_eff = n_eff, bdw = hmem[i], lbd = lambdat,etav)
res=rbind(res,rest)
######################update h and lambda
#sigma2 = (JUsquare/JUf - (Jmean_norm)^2) + rep(1,d) / sqrt(Nt[i])
#hthumb = (4/(d+2)) ** (1/(d+4)) * (Nt[i])^(-1/(d+4)) * sqrt(sigma2)
#}
plot(wn)
sum(wn>0)
res
i = i + 1
Jmean_norm
Jmean_current
beta = Jmean_norm
#  Jmean_current
#rbind(rep(1,31) , rep(-1,31)) * X_test[(1:2), ]
#beta = rep(1/d,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
#plot(xnew[1:1000,c(1)]+xnew[1:1000,c(2)])
#plot(xnew[1:1000,])
#plot(wn)
#etav
#etavKL
#etao
#plot(xnew[1:1000,c(1,2)] %*% rep(1,2))
#hist( xnew[1:1000,c(1,2)] %*% rep(1,2) )
#plot(xnew[wn<1,c(1,2)],col = "red")
#points(xnew[wn>1,c(1,2)])
source("log_reg.R")
#for (i in (1:Ttot)){
lambdat = lambdamem[i]
ht = rep(hmem[i],d)
#b = 1 / burnin
if (option == "new") {source("new_sampling.R")} else {source("expl_update_NPAIS_fast.R")}
#plot(wnorm)
#points(wnorm1)
######################save in a vector
rest=c(NPAIS_norm = err_norm , n_eff = n_eff, bdw = hmem[i], lbd = lambdat,etav)
res=rbind(res,rest)
######################update h and lambda
#sigma2 = (JUsquare/JUf - (Jmean_norm)^2) + rep(1,d) / sqrt(Nt[i])
#hthumb = (4/(d+2)) ** (1/(d+4)) * (Nt[i])^(-1/(d+4)) * sqrt(sigma2)
#}
plot(wn)
sum(wn>0)
res
i = i + 1
Jmean_norm
Jmean_current
beta = Jmean_norm
#  Jmean_current
#rbind(rep(1,31) , rep(-1,31)) * X_test[(1:2), ]
#beta = rep(1/d,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
#plot(xnew[1:1000,c(1)]+xnew[1:1000,c(2)])
#plot(xnew[1:1000,])
#plot(wn)
#etav
#etavKL
#etao
#plot(xnew[1:1000,c(1,2)] %*% rep(1,2))
#hist( xnew[1:1000,c(1,2)] %*% rep(1,2) )
#plot(xnew[wn<1,c(1,2)],col = "red")
#points(xnew[wn>1,c(1,2)])
source("log_reg.R")
#for (i in (1:Ttot)){
lambdat = lambdamem[i]
ht = rep(hmem[i],d)
#b = 1 / burnin
if (option == "new") {source("new_sampling.R")} else {source("expl_update_NPAIS_fast.R")}
#plot(wnorm)
#points(wnorm1)
######################save in a vector
rest=c(NPAIS_norm = err_norm , n_eff = n_eff, bdw = hmem[i], lbd = lambdat,etav)
res=rbind(res,rest)
######################update h and lambda
#sigma2 = (JUsquare/JUf - (Jmean_norm)^2) + rep(1,d) / sqrt(Nt[i])
#hthumb = (4/(d+2)) ** (1/(d+4)) * (Nt[i])^(-1/(d+4)) * sqrt(sigma2)
#}
plot(wn)
sum(wn>0)
res
i = i + 1
Jmean_norm
Jmean_current
beta = Jmean_norm
#  Jmean_current
#rbind(rep(1,31) , rep(-1,31)) * X_test[(1:2), ]
#beta = rep(1/d,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
#plot(xnew[1:1000,c(1)]+xnew[1:1000,c(2)])
#plot(xnew[1:1000,])
#plot(wn)
#etav
#etavKL
#etao
#plot(xnew[1:1000,c(1,2)] %*% rep(1,2))
#hist( xnew[1:1000,c(1,2)] %*% rep(1,2) )
#plot(xnew[wn<1,c(1,2)],col = "red")
#points(xnew[wn>1,c(1,2)])
source("log_reg.R")
#for (i in (1:Ttot)){
lambdat = lambdamem[i]
ht = rep(hmem[i],d)
#b = 1 / burnin
if (option == "new") {source("new_sampling.R")} else {source("expl_update_NPAIS_fast.R")}
#plot(wnorm)
#points(wnorm1)
######################save in a vector
rest=c(NPAIS_norm = err_norm , n_eff = n_eff, bdw = hmem[i], lbd = lambdat,etav)
res=rbind(res,rest)
######################update h and lambda
#sigma2 = (JUsquare/JUf - (Jmean_norm)^2) + rep(1,d) / sqrt(Nt[i])
#hthumb = (4/(d+2)) ** (1/(d+4)) * (Nt[i])^(-1/(d+4)) * sqrt(sigma2)
#}
plot(wn)
sum(wn>0)
res
i = i + 1
Jmean_norm
Jmean_current
beta = Jmean_norm
#  Jmean_current
#rbind(rep(1,31) , rep(-1,31)) * X_test[(1:2), ]
#beta = rep(1/d,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
#plot(xnew[1:1000,c(1)]+xnew[1:1000,c(2)])
#plot(xnew[1:1000,])
#plot(wn)
#etav
#etavKL
#etao
#plot(xnew[1:1000,c(1,2)] %*% rep(1,2))
#hist( xnew[1:1000,c(1,2)] %*% rep(1,2) )
#plot(xnew[wn<1,c(1,2)],col = "red")
#points(xnew[wn>1,c(1,2)])
mu_true
source("log_reg.R")
#for (i in (1:Ttot)){
lambdat = lambdamem[i]
ht = rep(hmem[i],d)
#b = 1 / burnin
if (option == "new") {source("new_sampling.R")} else {source("expl_update_NPAIS_fast.R")}
#plot(wnorm)
#points(wnorm1)
######################save in a vector
rest=c(NPAIS_norm = err_norm , n_eff = n_eff, bdw = hmem[i], lbd = lambdat,etav)
res=rbind(res,rest)
######################update h and lambda
#sigma2 = (JUsquare/JUf - (Jmean_norm)^2) + rep(1,d) / sqrt(Nt[i])
#hthumb = (4/(d+2)) ** (1/(d+4)) * (Nt[i])^(-1/(d+4)) * sqrt(sigma2)
#}
plot(wn)
sum(wn>0)
res
i = i + 1
Jmean_norm
Jmean_current
beta = Jmean_norm
#  Jmean_current
#rbind(rep(1,31) , rep(-1,31)) * X_test[(1:2), ]
#beta = rep(1/d,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
#plot(xnew[1:1000,c(1)]+xnew[1:1000,c(2)])
#plot(xnew[1:1000,])
#plot(wn)
#etav
#etavKL
#etao
#plot(xnew[1:1000,c(1,2)] %*% rep(1,2))
#hist( xnew[1:1000,c(1,2)] %*% rep(1,2) )
#plot(xnew[wn<1,c(1,2)],col = "red")
#points(xnew[wn>1,c(1,2)])
source("log_reg.R")
#for (i in (1:Ttot)){
lambdat = lambdamem[i]
ht = rep(hmem[i],d)
#b = 1 / burnin
if (option == "new") {source("new_sampling.R")} else {source("expl_update_NPAIS_fast.R")}
#plot(wnorm)
#points(wnorm1)
######################save in a vector
rest=c(NPAIS_norm = err_norm , n_eff = n_eff, bdw = hmem[i], lbd = lambdat,etav)
res=rbind(res,rest)
######################update h and lambda
#sigma2 = (JUsquare/JUf - (Jmean_norm)^2) + rep(1,d) / sqrt(Nt[i])
#hthumb = (4/(d+2)) ** (1/(d+4)) * (Nt[i])^(-1/(d+4)) * sqrt(sigma2)
#}
plot(wn)
sum(wn>0)
res
i = i + 1
Jmean_norm
Jmean_current
beta = Jmean_norm
#  Jmean_current
#rbind(rep(1,31) , rep(-1,31)) * X_test[(1:2), ]
#beta = rep(1/d,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
#plot(xnew[1:1000,c(1)]+xnew[1:1000,c(2)])
#plot(xnew[1:1000,])
#plot(wn)
#etav
#etavKL
#etao
#plot(xnew[1:1000,c(1,2)] %*% rep(1,2))
#hist( xnew[1:1000,c(1,2)] %*% rep(1,2) )
#plot(xnew[wn<1,c(1,2)],col = "red")
#points(xnew[wn>1,c(1,2)])
sum(wn>0)
etav
d
f
X_train
S
dim(X_train)
head(Xsam)
w
beta = rep(1/d,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
beta = c(rep(1/d,(d-1)),1)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
beta = rep(0,d)
temp = colSums(beta * t(X_test))
p1 = 1 / (1 + exp(- temp ) )
ypred = p1  > 0.5
mean(y_test == ypred)
